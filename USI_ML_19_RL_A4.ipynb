{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USI_ML_19_RL_A4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1kKJk_rJC34",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning 2019/2020: Assignment 4 -  Reinforcement Learning\n",
        "Deadline: Friday 6th of December 2019 9pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtJ0vlKJReh",
        "colab_type": "text"
      },
      "source": [
        "First name: Giorgia  \n",
        "Last name: Adorni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8xajf5MJ8Ud",
        "colab_type": "text"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "In this assignment you will further deepen your understanding of Reinforcement Learning (RL).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
        "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
        "Also select A3 for the PDF size to prevent content from being clipped.\n",
        "\n",
        "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
        "\n",
        "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
        "\n",
        "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
        "\n",
        "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "## How to get help\n",
        "\n",
        "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
        "However, do not look at any report written by others or share your report with others. Violation of that rule will result in 0 points for all students involved. For further questions you can send an email to louis@idsia.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3uo_XEzLkxc",
        "colab_type": "text"
      },
      "source": [
        "## 1 Basic probability (6p)\n",
        "\n",
        "Suppose that a migrating lizard that rests in Ticino can be in four different states:\n",
        "Eating (E), Sleeping (S), Fighting (F) and Mating (M), for example protecting its territory against other lizards. Each lizard spends 30% of its time sleeping, 40% eating, 20% fighting and the remaining time mating. A biologist collects a population of lizards and puts them in a cage to study their behaviors. Suppose the probability for a lizard being caught while eating is 0.1, for a sleeping lizard 0.4, for a fighting lizard 0.8 and for the lizards that are mating 0.2, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxN5sri9NNDm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRg4g3_NMXVI",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.1 (3p)\n",
        "What is the relative frequency (probability) for a lizard being caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$\\begin{aligned} \n",
        "P(\\mathrm{caught}) & = P(\\mathrm{caught}|\\mathrm{E})P(\\mathrm{E}) + P(\\mathrm{caught}|\\mathrm{S})P(\\mathrm{S}) + P(\\mathrm{caught}|\\mathrm{F})P(\\mathrm{F}) + P(\\mathrm{caught}|\\mathrm{M})P(\\mathrm{M}) =\\\\\n",
        "& = 0.1 \\cdot 0.4 + 0.4 \\cdot 0.3 + 0.8 \\cdot 0.2 + 0.2 \\cdot 0.1 = \\\\\n",
        "& = 0.04 + 0.12 + 0.16 + 0.02 = \\\\\n",
        "& = 0.34\n",
        "\\end{aligned}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OO7HpEwMdnH",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.2 (3p)\n",
        "\n",
        "What is the proportion of lizards that are fighting of those that were caught in the cage?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$P(\\mathrm{F}| \\mathrm{caught}) \\overset{\\mathrm{bayes}}{=} \\dfrac{P(\\mathrm{caught} | \\mathrm{F})P(\\mathrm{F})}{P(\\mathrm{caught})} = \\dfrac{0.16}{0.34} = 0.47$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJpynzHMpA6",
        "colab_type": "text"
      },
      "source": [
        "## 2 Markov Decision Processes (32p)\n",
        "\n",
        "Suppose a robot is put in a maze with long corridor. The\n",
        "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward for 1 meter, moving backward for 1 meter, turning left for 90 degrees and turning right for 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
        "**Note: the answers in the following questions should not exceed 5 sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3AATMANzC_",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.1 (4p)\n",
        "\n",
        "Assume the robot receives a +1 reward signal for each time step taken in the\n",
        "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The reward function is wrong. In fact, the robot is not encouraged to reach the end of the corridor, that is the final goal, since it is rewarded for each time step. In this way, for the robot could be better exploring the maze instad of reaching the final goal. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC-aYmiFOAEZ",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.2 (4p)\n",
        "\n",
        "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "As I said in the previous answer, the reward function is wrong. It is possible to correct it in the following way:\n",
        "\n",
        "\n",
        "*   not rewarding the robot for each time step or even penalising the time spent in the maze so as to encourage it to reach the end of the maze\n",
        "*   rewarding the robot only for the final goal\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIDv8qoOGuH",
        "colab_type": "text"
      },
      "source": [
        "### Questions 2.3 (2p)\n",
        "\n",
        "The discounted return for a non-episodic task is defined as\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
        "$$\n",
        "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n",
        "Rewrite the above equation such that $G_t$ is on the left hand side and $G_{t+1}$ is on the right hand side.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "Since $G_{t+1} = R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + \\ldots$, $G_t$ can be rewritten as:\n",
        "$$ \n",
        "G_{t} = R_{t+1}+\\gamma \\, G_{t+1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HTxEimD5O5eA"
      },
      "source": [
        "### Questions 2.4 (2p)\n",
        "\n",
        "What is the sufficient condition for this infinite series to be a convergent series?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "This infinite series is convergent if has a bounded reward sequence and $\\gamma , 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bp30j2csPJkz"
      },
      "source": [
        "### Questions 2.5 (5p)\n",
        "\n",
        "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The given infinite series $\\sum_{i=0}^{\\infty}\\gamma ^i R_{t+i+1}$ is a geometric series. Since each reward is a constant, it is possible to write $\\sum_{i=0}^{\\infty}\\gamma ^i \\cdot 1 = \\sum_{i=0}^{\\infty}\\gamma ^i$. Now, supposed thta the series is convergent, and so that $0 ≤ \\gamma < 1$, it converges to $\\dfrac{1}{1 -\\gamma}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bmDQKWHNPSnx"
      },
      "source": [
        "### Questions 2.6 (5p)\n",
        "\n",
        "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.3$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2, R_4 = 1, R_5 = 6$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$G_0 = R_1 + \\gamma \\, G_1 = -1 + 0.3 \\cdot 0.352 = -0.8944$\n",
        "\n",
        "$G_1 = R_2 + \\gamma \\, G_2 = -0.5 + 0.3 \\cdot 2.84 = 0.352$\n",
        "\n",
        "$G_2 = R_3 + \\gamma \\, G_3 = 2 + 0.3 \\cdot 2.8 = 2.84$\n",
        "\n",
        "$G_3 = R_4 + \\gamma \\, G_4 = 1 + 0.3 \\cdot 6 = 2.8$\n",
        "\n",
        "$G_4 = R_5 + \\gamma \\, G_5 = 6 + 0.3 \\cdot 0 = 6 $\n",
        "\n",
        "$G_5 = R_6 + \\gamma \\, G_6 = 0 $\n",
        "\n",
        "$G_5$ is 0 since $t>T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G7ZcAEvfQA8f"
      },
      "source": [
        "### Questions 2.7 (5p)\n",
        "\n",
        "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
        "Then how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$ G_t = \\sum_{i=0}^{\\infty} \\gamma ^ i (R_{t+1} + c) = \\sum_{i=0}^{\\infty} \\gamma ^ i R_{t+1} +  \\sum_{i=0}^{\\infty} \\gamma ^i c = G_t + \\sum_{i=0}^{\\infty} \\gamma ^i c = G_t + (c + \\gamma c+ \\gamma ^2 c + \\gamma ^3 c \\dots ) \\mbox{.}$$\n",
        "Hence, $G_t$ is increased by $\\sum_{i=0}^{\\infty} \\gamma ^i c = \\dfrac{c}{1-\\gamma}$. FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j7KPVdhPQBpz"
      },
      "source": [
        "### Questions 2.8 (5p)\n",
        "\n",
        "Now consider episodic tasks, and similar to Question 2.7, we add a constant $c$ to each reward, how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$ G_t = \\sum_{i=0}^{T-t-1} \\gamma ^ i (R_{t+1} + c) = \\sum_{i=0}^{T-t-1} \\gamma ^ i R_{t+1} +  \\sum_{i=0}^{T-t-1} \\gamma ^i c = G_t + \\sum_{i=0}^{T-t-1} \\gamma ^i c = G_t + (c + \\gamma c+ \\gamma ^2 c + \\gamma ^3 c \\dots \\gamma^{T-t-1} c) \\mbox{.}$$\n",
        "\n",
        "Hence, $G_t$ is increased by $\\sum_{i=0}^{T-t-1} \\gamma ^i c= c \\cdot \\dfrac{ 1 - \\gamma ^{T-t}}{1 -\\gamma} = \\dfrac{ c - c \\cdot \\gamma ^{T-t}}{1 -\\gamma} $, that is the amount of which $G_t$ is increased. FIXME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjOnM7AVQVOB",
        "colab_type": "text"
      },
      "source": [
        "## 3 Dynamic Programming (62p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Jm0EyIHQaOp"
      },
      "source": [
        "### Questions 3.1 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead. \n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$V^*(s)=\\max_\\pi V_\\pi(s)=\\max_a\\sum_{s',r}p(s',r|s,a)[r+\\gamma V^*\\pi(s')]\\quad \\forall s$$\n",
        "\n",
        "\n",
        "where:\n",
        "*   $a \\in A^{+}$ is the action\n",
        "*   $s \\in S^{+}$ is the state\n",
        "*   $r \\in R^{+}$ is the reward\n",
        "*   $\\pi \\in (a|s)$ is the policy\n",
        "*   $p(s',r|s,a)$ is FIXME\n",
        "*   $\\gamma$ is the discount factor\n",
        "*   $V_\\pi(s)$ is the state-value function for policy $\\pi$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f0VZcK6LQkUC"
      },
      "source": [
        "### Questions 3.2 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "$$Q^*(s,a)=\\max_\\pi Q_\\pi(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\max_{a'}Q^*(s', a')]\\quad \\forall s$$\n",
        "\n",
        "\n",
        "where:\n",
        "*   $a \\in A^{+}$ is an action and $a' \\in A^{+}$ is the following action\n",
        "*   $s \\in S^{+}$ is a state and $s' \\in S^{+}$ is the following state\n",
        "*   $r \\in R^{+}$ is the reward\n",
        "*   $\\pi \\in (a|s)$ is the policy\n",
        "*   $p(s',r|s,a)$ is the probability of each possible pair of next state an reward given any state and action \n",
        "*   $\\gamma$ is the discount factor\n",
        "*   $Q^*(s,a)$ is the action-value function for policy $\\pi$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tq66sRJeQlE2"
      },
      "source": [
        "### Questions 3.3 (15p)\n",
        "\n",
        "Consider a 4x4 gridworld depicted in the following table:\n",
        "\n",
        "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
        "\n",
        "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $\\bar S = \\{0, 15\\}$.\n",
        "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
        "Assume the state transitions are deterministic and all transitions result in a negative reward −1 (after termination all rewards are zero).\n",
        "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
        "Note: In this exercise, we assume the policy is a deterministic\n",
        "function.\n",
        "\n",
        "Manually run the policy iteration algorithm (see lecture slide 58) for one iteration. Use the in-place policy iteration algorithm.\n",
        "This means one time policy evaluation with a single pass through the states (16 equations) and one time policy improvement.\n",
        "Assume the initial state value for all 16 cells are 0.0 and the policy initially always outputs the 'left' action.\n",
        "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
        "Use a discount factor $\\gamma = 0.5$.\n",
        "Write down the policy after policy improvement.\n",
        "\n",
        "![Policy iteration](http://www.incompleteideas.net/book/ebook/imgtmp5.png)\n",
        "\n",
        "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "*Policy Evaluation*\n",
        "\n",
        "$$\n",
        "V(s) \\leftarrow \\sum_a \\pi(s,a) \\sum_{s', r} p(s', r | s, a) [r + \\gamma V(s')]\n",
        "$$\n",
        "\n",
        "It has to be noticed that, since the initial policy always returns the \"left\" action, both the sum are simplified because just a single $\\pi(s,a)$ and $\\mathcal{P}_{ss'}^a$ will be equal to 1, while the other will be 0. The following are the already semplified versions of the formula.\n",
        "\n",
        "$$ \\begin{aligned} &V(0) \\leftarrow 1 \\cdot [0 + 0.5 \\cdot 0] = 0 \\qquad &\\text{where } a = \\text{left}, s' = 0  \\\\\n",
        "&V(1) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &\\text{where } a = \\text{left}, s' = 0  \\\\\n",
        "&V(2) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &\\text{where } a = \\text{left}, s' = 1  \\\\\n",
        "&V(3) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &\\text{where } a = \\text{left}, s' = 2  \\\\\n",
        "&V(4) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &\\text{where } a = \\text{left}, s' = 4  \\\\\n",
        "&V(5) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &\\text{where } a = \\text{left}, s' = 4  \\\\\n",
        "&V(6) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &\\text{where } a = \\text{left}, s' = 5  \\\\\n",
        "&V(7) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.75] = -1.875 \\qquad &\\text{where } a = \\text{left}, s' = 6  \\\\\n",
        "&V(8) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &\\text{where } a = \\text{left}, s' = 8  \\\\\n",
        "&V(9) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &\\text{where } a = \\text{left}, s' = 8  \\\\\n",
        "&V(10) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &\\text{where } a = \\text{left}, s' = 9  \\\\\n",
        "&V(11) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.75] = -1.875 \\qquad &\\text{where } a = \\text{left}, s' = 10  \\\\\n",
        "&V(12) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot 0] = -1 \\qquad &\\text{where } a = \\text{left}, s' = 12  \\\\\n",
        "&V(13) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1] = -1.5 \\qquad &\\text{where } a = \\text{left}, s' = 12  \\\\\n",
        "&V(14) \\leftarrow 1 \\cdot [-1 + 0.5 \\cdot -1.5] = -1.75 \\qquad &\\text{where } a = \\text{left}, s' = 13  \\\\\n",
        "&V(15) \\leftarrow 1 \\cdot [0 + 0.5 \\cdot -1.75] = -0.875 \\qquad &\\text{where } a = \\text{left}, s' = 15 \\end{aligned}$$\n",
        "\n",
        "*Policy Improvement*\n",
        "\n",
        "$$\n",
        "\\pi'(s) = \\arg \\max_a Q_\\pi (s,a) = \\arg \\max_a \\sum_{s', r} p(s', r | s, a) [r + \\gamma V_\\pi(s')]\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (0) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(0+0.5(0))=0; & 1(0+0.5(0))=0; & 1(0+0.5(0))=0; & 1(0+0.5(0))=0 \\\\\n",
        "    a=\\mathrm{up}, s'=0 & a=\\mathrm{down}, s'=0 & a=\\mathrm{left}, s'=0 & a=\\mathrm{right}, s'=0 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (1) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(0+0.5(-1))=-1.5; & 1(0+0.5(-1.5))=-1.75; & 1(0+0.5(0))=-1; & 1(0+0.5(-1.5))=-1.75 \\\\\n",
        "    a=\\mathrm{up}, s'=0 & a=\\mathrm{down}, s'=0 & a=\\mathrm{left}, s'=0 & a=\\mathrm{right}, s'=0 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (2) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875 \\\\\n",
        "    a=\\mathrm{up}, s'=2 & a=\\mathrm{down}, s'=6 & a=\\mathrm{left}, s'=1 & a=\\mathrm{right}, s'=3 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (3) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.875))=-1.9375; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.75))=-1.875 \\\\\n",
        "    a=\\mathrm{up}, s'=3 & a=\\mathrm{down}, s'=7 & a=\\mathrm{left}, s'=2 & a=\\mathrm{right}, s'=3 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (4) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(0))=-1; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.5))=-1.75 \\\\\n",
        "    a=\\mathrm{up}, s'=0 & a=\\mathrm{down}, s'=8 & a=\\mathrm{left}, s'=4 & a=\\mathrm{right}, s'=5 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (5) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875 \\\\\n",
        "    a=\\mathrm{up}, s'=1 & a=\\mathrm{down}, s'=9 & a=\\mathrm{left}, s'=4 & a=\\mathrm{right}, s'=6 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (6) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.875))=-1.9375 \\\\\n",
        "    a=\\mathrm{up}, s'=2 & a=\\mathrm{down}, s'=10 & a=\\mathrm{left}, s'=5 & a=\\mathrm{right}, s'=7 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (7) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.875))=-1.9375; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.875))=-1.9375 \\\\\n",
        "    a=\\mathrm{up}, s'=3 & a=\\mathrm{down}, s'=11 & a=\\mathrm{left}, s'=6 & a=\\mathrm{right}, s'=7 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (8) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.5))=-1.75 \\\\\n",
        "    a=\\mathrm{up}, s'=4 & a=\\mathrm{down}, s'=12 & a=\\mathrm{left}, s'=8 & a=\\mathrm{right}, s'=9 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (9) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875 \\\\\n",
        "    a=\\mathrm{up}, s'=5 & a=\\mathrm{down}, s'=13 & a=\\mathrm{left}, s'=8 & a=\\mathrm{right}, s'=10 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (10) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.875))=-1.9375 \\\\\n",
        "    a=\\mathrm{up}, s'=6 & a=\\mathrm{down}, s'=14 & a=\\mathrm{left}, s'=9 & a=\\mathrm{right}, s'=11 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (11) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.875))=-1.9375; & 1(-1+0.5(0))=-1; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.875))=-1.9375 \\\\\n",
        "    a=\\mathrm{up}, s'=7 & a=\\mathrm{down}, s'=15 & a=\\mathrm{left}, s'=10 & a=\\mathrm{right}, s'=11 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (12) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.5))=-1.75 \\\\\n",
        "    a=\\mathrm{up}, s'=8 & a=\\mathrm{down}, s'=12 & a=\\mathrm{left}, s'=12 & a=\\mathrm{right}, s'=13 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (13) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5(-1))=-1.5; & 1(-1+0.5(-1.75))=-1.875 \\\\\n",
        "    a=\\mathrm{up}, s'=9 & a=\\mathrm{down}, s'=13 & a=\\mathrm{left}, s'=12 & a=\\mathrm{right}, s'=14 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (14) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.75))=-1.875; & 1(-1+0.5(-1.5))=-1.75; & 1(-1+0.5( -0.875))=-1.4375 \\\\\n",
        "    a=\\mathrm{up}, s'=10 & a=\\mathrm{down}, s'=14 & a=\\mathrm{left}, s'=13 & a=\\mathrm{right}, s'=15 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n",
        "$$ \n",
        "{\\pi ' (15) = \\arg\\max_a \n",
        "\\begin{pmatrix}\\begin{Bmatrix}\n",
        "    1(0+0.5(-1.875))=-1.9375; & 1(0+0.5(-0.875))=-0.4375; & 1(0+0.5(-1.75))=-1.875; & 1(0+0.5(-0.875))=-0.4375 \\\\\n",
        "    a=\\mathrm{up}, s'=11 & a=\\mathrm{down}, s'=15 & a=\\mathrm{left}, s'=14 & a=\\mathrm{right}, s'=15 \n",
        "\\end{Bmatrix} \\end{pmatrix} }\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ciourK1wQo-3"
      },
      "source": [
        "### Questions 3.4 (15p)\n",
        "\n",
        "Implement the before mentioned environment in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gmuwlmdyO4",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzicBh-I3dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=180)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_bNpgqd2mM",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBoBp3PJC0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GridWorld:\n",
        "    UP = 0\n",
        "    DOWN = 1\n",
        "    LEFT = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "    def __init__(self, side=4):\n",
        "        self.side = side\n",
        "        # -------------------------\n",
        "        # Define integer states, actions, and final states as specified in the problem description\n",
        "\n",
        "        # TODO insert code here\n",
        "        self.actions = np.array([self.UP, self.DOWN, self.LEFT, self.RIGHT])\n",
        "        self.states = np.arange(16)\n",
        "        self.finals = np.array([0, 15])\n",
        "\n",
        "        # -------------------------\n",
        "        self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "\n",
        "    def reward(self, s, s_next, a):\n",
        "        # -------------------------\n",
        "        # Return the reward for the given transition as specified in the problem description\n",
        "\n",
        "        # TODO insert code here\n",
        "        reward = -1\n",
        "        if s in self.finals:\n",
        "            reward = 0\n",
        "\n",
        "        return reward\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def transition_prob(self, s, s_next, a):\n",
        "        # -------------------------\n",
        "        # Return a probability in [0, 1] for the given transition as specified in the problem description\n",
        "\n",
        "        # TODO insert code here\n",
        "        prob = 0\n",
        "\n",
        "        if s in self.finals:\n",
        "            if s == s_next:\n",
        "                prob = 1\n",
        "            else:\n",
        "                prob = 0\n",
        "        elif (a == self.UP and (s - self.side == s_next or (s < self.side and s == s_next))) or \\\n",
        "                (a == self.DOWN and (s + self.side == s_next or (s >= self.side * self.side - self.side and s == s_next))) or \\\n",
        "                (a == self.LEFT and (s - 1 == s_next or (s % self.side == 0 and s == s_next))) or \\\n",
        "                (a == self.RIGHT and (s + 1 == s_next or ((s + 1) % self.side == 0 and s == s_next))):\n",
        "            prob = 1\n",
        "\n",
        "        return prob\n",
        "        # -------------------------\n",
        "\n",
        "    def print_policy(self, policy):\n",
        "        P = np.array(policy).reshape(self.side, self.side)\n",
        "        print(self.actions_repr[P])\n",
        "\n",
        "    def print_values(self, values):\n",
        "        V = np.array(values).reshape(self.side, self.side)\n",
        "        print(V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FqMt-0yAuGz8"
      },
      "source": [
        "### Questions 3.5 (17p)\n",
        "\n",
        "Implement policy iteration in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "\n",
        "Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "yes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUMxh-qd5u0",
        "colab_type": "text"
      },
      "source": [
        "#### Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1DOXcH5J0NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_policy(world, policy, values, gamma=0.9, theta=0.01):\n",
        "    # --------------------------\n",
        "    # Implement policy evaluation and return the updated value function\n",
        "\n",
        "    # TODO insert code here\n",
        "    while True:\n",
        "        delta = 0.0\n",
        "        old_values = np.copy(values)\n",
        "\n",
        "        for s in world.states:\n",
        "            a = policy[s]\n",
        "            v = np.zeros_like(world.states, dtype=np.float32)\n",
        "\n",
        "            for s_next in world.states:\n",
        "                p = world.transition_prob(s, s_next, a)\n",
        "                r = world.reward(s, s_next, a)\n",
        "                v[s_next] = p * (r + gamma * values[s_next])\n",
        "\n",
        "            values[s] = np.sum(v)\n",
        "            delta = np.maximum(delta, np.absolute(values[s] - old_values[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return values\n",
        "    # --------------------------\n",
        "\n",
        "\n",
        "def improve_policy(world, policy, values, gamma=0.9):\n",
        "    # --------------------------\n",
        "    # Implement policy improvement and return the updated policy\n",
        "\n",
        "    # TODO insert code here\n",
        "    policy_stable = True\n",
        "\n",
        "    for s in world.states:\n",
        "        argmax_q = -np.inf\n",
        "        best_a = 0\n",
        "\n",
        "        for a in world.actions:\n",
        "            q_actual = 0\n",
        "\n",
        "            for s_next in world.states:\n",
        "                p = world.transition_prob(s, s_next, a)\n",
        "                r = world.reward(s, s_next, a)\n",
        "                q_actual += p * (r + gamma * values[s_next])\n",
        "\n",
        "            if q_actual > argmax_q:\n",
        "                argmax_q = q_actual\n",
        "                best_a = a\n",
        "\n",
        "        if policy[s] != best_a:\n",
        "            policy_stable = False\n",
        "\n",
        "        policy[s] = best_a\n",
        "\n",
        "    return policy_stable\n",
        "    # --------------------------\n",
        "\n",
        "\n",
        "def policy_iteration(world, gamma=0.9, theta=0.01):\n",
        "    # Initialize a random policy\n",
        "    policy = np.array([np.random.choice(world.actions) for s in world.states])\n",
        "    print('Initial policy')\n",
        "    world.print_policy(policy)\n",
        "    # Initialize values to zero\n",
        "    values = np.zeros_like(world.states, dtype=np.float32)\n",
        "\n",
        "    # Run policy iteration\n",
        "    stable = False\n",
        "    for i in itertools.count():\n",
        "        print(f'Iteration {i}')\n",
        "        values = eval_policy(world, policy, values, gamma, theta)\n",
        "        world.print_values(values)\n",
        "        stable = improve_policy(world, policy, values, gamma)\n",
        "        world.print_policy(policy)\n",
        "        if stable:\n",
        "            break\n",
        "\n",
        "    return policy, values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqdoNw97mcEA",
        "colab_type": "code",
        "outputId": "faa526bf-27f9-4d07-8a11-3b4b3a3f12d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "# Evaluate your code, please include the output in your submission\n",
        "world = GridWorld()\n",
        "policy, values = policy_iteration(world, gamma=0.5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial policy\n",
            "[['→' '→' '↓' '←']\n",
            " ['↑' '↑' '↓' '←']\n",
            " ['←' '↑' '↓' '↓']\n",
            " ['↓' '←' '↓' '↓']]\n",
            "Iteration 0\n",
            "[[ 0.    -1.998 -1.998 -1.999]\n",
            " [-1.    -1.999 -1.998 -1.999]\n",
            " [-5.989 -2.    -1.998 -1.   ]\n",
            " [-1.998 -1.999 -1.998  0.   ]]\n",
            "[['↑' '←' '↑' '←']\n",
            " ['↑' '←' '↑' '↓']\n",
            " ['↑' '→' '→' '↓']\n",
            " ['↓' '←' '→' '↑']]\n",
            "Iteration 1\n",
            "[[ 0.   -1.   -2.   -2.  ]\n",
            " [-1.   -1.5  -2.   -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-2.   -2.   -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n",
            "Iteration 2\n",
            "[[ 0.   -1.   -1.5  -1.75]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n",
            "Iteration 3\n",
            "[[ 0.   -1.   -1.5  -1.75]\n",
            " [-1.   -1.5  -1.75 -1.5 ]\n",
            " [-1.5  -1.75 -1.5  -1.  ]\n",
            " [-1.75 -1.5  -1.    0.  ]]\n",
            "[['↑' '←' '←' '↓']\n",
            " ['↑' '↑' '↑' '↓']\n",
            " ['↑' '↑' '↓' '↓']\n",
            " ['↑' '→' '→' '↑']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1hRFm6Zm6YI1"
      },
      "source": [
        "### Questions 3.5 (5p)\n",
        "\n",
        "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n",
        "\n",
        "The problem is that is not guaranteed the convergence of the series when $\\gamma = 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1rAQ1K_u6qtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "ec148759-29a6-48dc-a862-9a47cf7c69b1"
      },
      "source": [
        "policy_iteration(world, gamma=1.0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial policy\n",
            "[['↓' '→' '←' '←']\n",
            " ['↑' '←' '→' '↑']\n",
            " ['←' '↑' '↓' '←']\n",
            " ['←' '↓' '→' '←']]\n",
            "Iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-88ea85a7e650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-5887b99860c1>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(world, gamma, theta)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Iteration {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mstable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimprove_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5887b99860c1>\u001b[0m in \u001b[0;36meval_policy\u001b[0;34m(world, policy, values, gamma, theta)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0ms_next\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}